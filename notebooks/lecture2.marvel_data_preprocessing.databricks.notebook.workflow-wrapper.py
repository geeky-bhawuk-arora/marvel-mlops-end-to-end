# Databricks notebook source
import os
os.chdir(os.path.dirname('/Workspace/Users/bhawukarora042@gmail.com/.bundle/dev/marvel-characters/files/notebooks/lecture2.marvel_data_preprocessing.py'))
%pip install -e ..

# COMMAND ----------

# This cell is autogenerated by the Databricks Extension for VS Code
def databricks_preamble():
    from IPython import get_ipython
    from typing import List
    from shlex import quote
    import os
    import sys

    src_file_dir = os.path.dirname("/Workspace/Users/bhawukarora042@gmail.com/.bundle/dev/marvel-characters/files/notebooks/lecture2.marvel_data_preprocessing.py")
    os.chdir(src_file_dir)

    project_root_dir = "/Workspace/Users/bhawukarora042@gmail.com/.bundle/dev/marvel-characters/files"
    sys.path.insert(0, project_root_dir)

    def parse_databricks_magic_lines(lines: List[str]):
        if len(lines) == 0:
            return lines

        first = ""
        for line in lines:
            if len(line.strip()) != 0:
                first = line
                break

        if first.startswith("%"):
            magic = first.split(" ")[0].strip().strip("%")
            rest = ' '.join(first.split(" ")[1:])

            if magic == "sh":
                return [
                    "%sh\n",
                    f"cd {quote(src_file_dir)}\n",
                    rest.strip() + "\n",
                    *lines[1:]
                ]

        return lines

    ip = get_ipython()
    ip.input_transformers_cleanup.append(parse_databricks_magic_lines)


try:
    databricks_preamble()
    del databricks_preamble
except Exception as e:
    print("Error in databricks_preamble: " + str(e))

# COMMAND ----------

# DBTITLE 1,Cell 1
# %restart_python

# COMMAND ----------

# MAGIC %restart_python

# COMMAND ----------

from pathlib import Path
import sys
sys.path.append(str(Path.cwd().parent / 'src'))

# COMMAND ----------

import pandas as pd
import yaml
from loguru import logger
from pyspark.sql import SparkSession

from marvel_characters.config import ProjectConfig
from marvel_characters.data_processor import DataProcessor

config = ProjectConfig.from_yaml(config_path="../project_config_marvel.yml", env="dev")

logger.info("Configuration loaded:")
logger.info(yaml.dump(config, default_flow_style=False))

# COMMAND ----------

# Load the Marvel characters dataset
spark = SparkSession.builder.getOrCreate()

filepath = "../data/marvel_characters_dataset.csv"

# Load the data
df = pd.read_csv(filepath)

# Display basic info about the dataset
logger.info(f"Dataset shape: {df.shape}")
logger.info(f"Columns: {list(df.columns)}")
logger.info(f"Target column '{config.target}' distribution:")
logger.info(df[config.target].value_counts())

# COMMAND ----------

# Load the Marvel characters dataset

data_processor = DataProcessor(df, config, spark)

# Preprocess the data
data_processor.preprocess()

logger.info(f"Data preprocessing completed.")

# COMMAND ----------

# Split the data
X_train, X_test = data_processor.split_data()
logger.info("Training set shape: %s", X_train.shape)
logger.info("Test set shape: %s", X_test.shape)

# COMMAND ----------

# Save to catalog
logger.info("Saving data to catalog")
data_processor.save_to_catalog(X_train, X_test)

# Enable change data feed (only once!)
logger.info("Enable change data feed")
data_processor.enable_change_data_feed()
