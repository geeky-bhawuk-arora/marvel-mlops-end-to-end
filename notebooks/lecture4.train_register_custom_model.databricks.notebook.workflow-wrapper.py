# Databricks notebook source
# This cell is autogenerated by the Databricks Extension for VS Code
def databricks_preamble():
    from IPython import get_ipython
    from typing import List
    from shlex import quote
    import os
    import sys

    src_file_dir = os.path.dirname("/Workspace/Users/bhawukarora042@gmail.com/.bundle/dev/marvel-characters/files/notebooks/lecture4.train_register_custom_model.py")
    os.chdir(src_file_dir)

    project_root_dir = "/Workspace/Users/bhawukarora042@gmail.com/.bundle/dev/marvel-characters/files"
    sys.path.insert(0, project_root_dir)

    def parse_databricks_magic_lines(lines: List[str]):
        if len(lines) == 0:
            return lines

        first = ""
        for line in lines:
            if len(line.strip()) != 0:
                first = line
                break

        if first.startswith("%"):
            magic = first.split(" ")[0].strip().strip("%")
            rest = ' '.join(first.split(" ")[1:])

            if magic == "sh":
                return [
                    "%sh\n",
                    f"cd {quote(src_file_dir)}\n",
                    rest.strip() + "\n",
                    *lines[1:]
                ]

        return lines

    ip = get_ipython()
    ip.input_transformers_cleanup.append(parse_databricks_magic_lines)


try:
    databricks_preamble()
    del databricks_preamble
except Exception as e:
    print("Error in databricks_preamble: " + str(e))

# COMMAND ----------

import mlflow
from pyspark.sql import SparkSession

from marvel_characters.config import ProjectConfig, Tags
from marvel_characters.models.custom_model import MarvelModelWrapper
from importlib.metadata import version
from dotenv import load_dotenv
from mlflow import MlflowClient
import os

# Set up Databricks or local MLflow tracking
def is_databricks():
    return "DATABRICKS_RUNTIME_VERSION" in os.environ

# COMMAND ----------

# If you have DEFAULT profile and are logged in with DEFAULT profile,
# skip these lines

if not is_databricks():
    load_dotenv()
    profile = os.environ["PROFILE"]
    mlflow.set_tracking_uri(f"databricks://{profile}")
    mlflow.set_registry_uri(f"databricks-uc://{profile}")


config = ProjectConfig.from_yaml(config_path="../project_config_marvel.yml", env="dev")
spark = SparkSession.builder.getOrCreate()
tags = Tags(**{"git_sha": "abcd12345", "branch": "main"})
marvel_characters_v = version("marvel_characters")

code_paths=[f"../dist/marvel_characters-{marvel_characters_v}-py3-none-any.whl"]

# COMMAND ----------

client = MlflowClient()
wrapped_model_version = client.get_model_version_by_alias(
    name=f"{config.catalog_name}.{config.schema_name}.marvel_character_model_basic",
    alias="latest-model")
# Initialize model with the config path

# COMMAND ----------

test_set = spark.table(f"{config.catalog_name}.{config.schema_name}.test_set").toPandas()
X_test = test_set[config.num_features + config.cat_features]

# COMMAND ----------

pyfunc_model_name = f"{config.catalog_name}.{config.schema_name}.marvel_character_model_custom"
wrapper = MarvelModelWrapper()
wrapper.log_register_model(wrapped_model_uri=f"models:/{wrapped_model_version.model_id}",
                           pyfunc_model_name=pyfunc_model_name,
                           experiment_name=config.experiment_name_custom,
                           input_example=X_test[0:1],
                           tags=tags,
                           code_paths=code_paths)

# COMMAND ----------

# unwrap and predict
loaded_pufunc_model = mlflow.pyfunc.load_model(f"models:/{pyfunc_model_name}@latest-model")

unwraped_model = loaded_pufunc_model.unwrap_python_model()

# COMMAND ----------

unwraped_model.predict(context=None, model_input=X_test[0:1])

# COMMAND ----------

# another predict function with uri

loaded_pufunc_model.predict(X_test[0:1])
